{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1bd638-95f6-458b-bf5b-86a952744b81",
   "metadata": {},
   "source": [
    "# Обучение трансформеров для классификации токсичных комментариев"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ce8a4-1e34-4d6f-9359-d714c27f5591",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b35c7809-bbc2-40ea-9aa2-dee2ca4341d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EvalPrediction\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5917851e-f63e-4524-a3ce-df4eecbaae92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c325c6-fad8-42fe-b52c-b8ac505ac813",
   "metadata": {},
   "source": [
    "## Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700ff2ca-9028-4f5d-acd1-284ebe674b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('modified_train.csv', usecols=['comment_text', 'toxicity_b', 'created_date'],parse_dates=['created_date'],\n",
    "    date_format='ISO8601')\n",
    "df = df[df['created_date'] >= '2016-04-01']\n",
    "df = df.sort_values(by='created_date')[['comment_text', 'toxicity_b']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7e374-cec1-49ec-8cbb-13a458ea6b6b",
   "metadata": {},
   "source": [
    "### Очистка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e70e4f8-b308-4455-b572-77768bae942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_re = re.compile(r'(https?://\\S+|www\\.\\S+)', re.IGNORECASE)\n",
    "spaces_re = re.compile(r'\\s+')\n",
    "\n",
    "def clean_text_roberta(text):\n",
    "    # Удаление ссылок\n",
    "    text = url_re.sub('', text)\n",
    "    # Очистка лишних пробелов\n",
    "    text = spaces_re.sub(' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46327a2b-f202-4c37-b0e2-33db0b7689bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111f07404b4a4168acfd3aa4b219dbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=297552), Label(value='0 / 297552')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['comment_text'] = df['comment_text'].parallel_apply(clean_text_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27afff31-538a-406b-be98-89a342b6cedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If there is a special session,it should be hel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As they should my friend, as they should. It's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just got back from the 5th Ave mall and was pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why bother going into Dutch, Just run the Trus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sounds like a lot of stereotyping going on, CS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxicity_b\n",
       "0  If there is a special session,it should be hel...           0\n",
       "1  As they should my friend, as they should. It's...           0\n",
       "2  Just got back from the 5th Ave mall and was pa...           0\n",
       "3  Why bother going into Dutch, Just run the Trus...           0\n",
       "4  Sounds like a lot of stereotyping going on, CS...           0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097f2bb-ef7a-4c81-b421-86b3a2227bc7",
   "metadata": {},
   "source": [
    "### Разделение данных на Train-Val-Test и подготовка данных для использования в моделях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e987902e-e49b-43b2-900a-ecef9742b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.texts = dataframe['comment_text'].values\n",
    "        self.labels = dataframe['toxicity_b'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95b6330-cc44-4fc5-942f-55432e483773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_train_metrics(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(p.predictions), dim=-1).numpy()[:, 1]\n",
    "    \n",
    "    return {\n",
    "        'roc_auc': roc_auc_score(p.label_ids, probs),\n",
    "        'pr_auc': average_precision_score(p.label_ids, probs),\n",
    "        'f1': f1_score(p.label_ids, preds),\n",
    "        'accuracy': accuracy_score(p.label_ids, preds),\n",
    "        'precision': precision_score(p.label_ids, preds),\n",
    "        'recall': recall_score(p.label_ids, preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7ccdd35-8046-40d2-ab71-a7fe19bc2f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers(model, num_frozen_layers=18):\n",
    "    for param in model.roberta.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for i in range(num_frozen_layers):\n",
    "        for param in model.roberta.encoder.layer[i].parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31baffba-a73b-4623-9a06-494c38bdf48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.loc[:79999], df.loc[80000:]\n",
    "train_df, val_df = train_df.loc[:63999], train_df.loc[64000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65420895-78fa-4bac-8afe-70dbc44d3af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxicity_b\n",
       "0    0.904375\n",
       "1    0.095625\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.toxicity_b.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa1a28a5-5cf7-479c-b527-5c1cdee9459b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxicity_b\n",
       "0    0.888563\n",
       "1    0.111437\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.toxicity_b.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d97c17-9e98-4572-bd13-1e3fe7b83bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxicity_b\n",
       "0    0.88749\n",
       "1    0.11251\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.toxicity_b.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72469b2c-d228-42cb-9e77-13c27671d0f7",
   "metadata": {},
   "source": [
    "## Обучение моделей на 100K наблюдениях"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c3b6e-9b7b-4198-b06f-049bc9621eb8",
   "metadata": {},
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8e7355bb-2031-4924-8ba7-aa1fbd443c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dbert = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6fc08ed-1487-4df8-968b-0544ba8daa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df, tokenizer_dbert)\n",
    "val_dataset = TextDataset(val_df, tokenizer_dbert)\n",
    "test_dataset = TextDataset(test_df, tokenizer_dbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b25b25ae-27d5-4a06-954d-a14bfa2c66c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_dbert = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c25eeb18-e959-4350-a4c1-5d5a0510e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./dbert_training_results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    metric_for_best_model='f1',\n",
    "    fp16=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c126006e-b457-44ca-a104-56badc275a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_dbert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_train_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23577ab1-5c29-4126-bf2e-1e9383da614f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 15:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.154400</td>\n",
       "      <td>0.178868</td>\n",
       "      <td>0.942403</td>\n",
       "      <td>0.737331</td>\n",
       "      <td>0.657282</td>\n",
       "      <td>0.925438</td>\n",
       "      <td>0.673734</td>\n",
       "      <td>0.641615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.199488</td>\n",
       "      <td>0.942881</td>\n",
       "      <td>0.749698</td>\n",
       "      <td>0.667223</td>\n",
       "      <td>0.925250</td>\n",
       "      <td>0.662065</td>\n",
       "      <td>0.672462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.227283</td>\n",
       "      <td>0.942714</td>\n",
       "      <td>0.749201</td>\n",
       "      <td>0.661359</td>\n",
       "      <td>0.925562</td>\n",
       "      <td>0.670704</td>\n",
       "      <td>0.652271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.12615878677368164, metrics={'train_runtime': 935.3046, 'train_samples_per_second': 205.281, 'train_steps_per_second': 6.415, 'total_flos': 2.5433740541952e+16, 'train_loss': 0.12615878677368164, 'epoch': 3.0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6452ce0e-9a6b-4f06-9700-40fe38ebb215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae8845d4-f92d-440b-bef1-531f90793558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший порог: 0.2820\n",
      "Лучший F1: 0.6700\n"
     ]
    }
   ],
   "source": [
    "y_val_np = val_df['toxicity_b'].to_numpy()\n",
    "preds = trainer.predict(val_dataset)\n",
    "probs = softmax(preds.predictions, axis=1)[:, 1]\n",
    "\n",
    "\n",
    "thresholds = np.linspace(probs.min(), probs.max(), 100)\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    binary_preds = (probs > threshold).astype(int)\n",
    "    f1 = f1_score(y_val_np, binary_preds)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Лучший порог: {best_threshold:.4f}\")\n",
    "print(f\"Лучший F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b89a92e3-c421-4728-a73f-21fb06f9d942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6613591128802957"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_preds = (probs > 0.5).astype(int)\n",
    "f1_score(y_val_np, binary_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994fb64-32e8-47e3-99cd-854016b7d089",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a82b12fb-719f-42b1-a460-6561086d2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4651bf73-2e17-4a1e-a98d-d9cd4f1ad7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df, tokenizer_bert)\n",
    "val_dataset = TextDataset(val_df, tokenizer_bert)\n",
    "test_dataset = TextDataset(test_df, tokenizer_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d79cd404-f896-4a99-91b3-6aec69dd9b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_bert = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "96f1c20b-80a0-4d1e-96e8-b6d1346f077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert_training_results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    metric_for_best_model='f1',\n",
    "    fp16=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "471423c3-027c-4162-bb1c-c9598925b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_train_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5c40b1e7-e2cb-4216-a874-df2ebbe885dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 28:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>0.178491</td>\n",
       "      <td>0.944058</td>\n",
       "      <td>0.743018</td>\n",
       "      <td>0.651759</td>\n",
       "      <td>0.927000</td>\n",
       "      <td>0.695735</td>\n",
       "      <td>0.613012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.111900</td>\n",
       "      <td>0.202396</td>\n",
       "      <td>0.944740</td>\n",
       "      <td>0.756716</td>\n",
       "      <td>0.669485</td>\n",
       "      <td>0.921813</td>\n",
       "      <td>0.632867</td>\n",
       "      <td>0.710600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.250403</td>\n",
       "      <td>0.942726</td>\n",
       "      <td>0.751160</td>\n",
       "      <td>0.667044</td>\n",
       "      <td>0.926375</td>\n",
       "      <td>0.672365</td>\n",
       "      <td>0.661806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.12013796742757162, metrics={'train_runtime': 1711.9156, 'train_samples_per_second': 112.155, 'train_steps_per_second': 3.505, 'total_flos': 5.051732262912e+16, 'train_loss': 0.12013796742757162, 'epoch': 3.0})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f7cfeb4f-8b71-4566-b19f-c5f9635f35b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший порог: 0.2925\n",
      "Лучший F1: 0.6708\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(val_dataset)\n",
    "probs = softmax(preds.predictions, axis=1)[:, 1]\n",
    "\n",
    "\n",
    "thresholds = np.linspace(probs.min(), probs.max(), 100)\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    binary_preds = (probs > threshold).astype(int)\n",
    "    f1 = f1_score(y_val_np, binary_preds)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Лучший порог: {best_threshold:.4f}\")\n",
    "print(f\"Лучший F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f8d85-866d-4a5b-b60f-27e970b304b1",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "308330a4-10d5-4018-b911-d9167466d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_roberta = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5f265f77-50af-44bb-969a-96c05cf16947",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df, tokenizer_roberta)\n",
    "val_dataset = TextDataset(val_df, tokenizer_roberta)\n",
    "test_dataset = TextDataset(test_df, tokenizer_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7f8e6085-ce0e-410c-beb0-0fc7f1315c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_roberta = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0f7fc06e-9dc5-458a-8f46-4f09f7e36157",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./roberta_training_results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    metric_for_best_model='f1',\n",
    "    fp16=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2e6a3555-e40a-4825-a55b-9c52397e1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_roberta,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_train_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b72edb0f-c76b-49e0-a18c-d7cce12647b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 29:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.160500</td>\n",
       "      <td>0.197114</td>\n",
       "      <td>0.946548</td>\n",
       "      <td>0.742811</td>\n",
       "      <td>0.660901</td>\n",
       "      <td>0.916687</td>\n",
       "      <td>0.604749</td>\n",
       "      <td>0.728547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.122800</td>\n",
       "      <td>0.200790</td>\n",
       "      <td>0.946032</td>\n",
       "      <td>0.754868</td>\n",
       "      <td>0.665073</td>\n",
       "      <td>0.921188</td>\n",
       "      <td>0.631685</td>\n",
       "      <td>0.702187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.094700</td>\n",
       "      <td>0.222156</td>\n",
       "      <td>0.945871</td>\n",
       "      <td>0.752541</td>\n",
       "      <td>0.667756</td>\n",
       "      <td>0.923750</td>\n",
       "      <td>0.649021</td>\n",
       "      <td>0.687605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.13461771583557128, metrics={'train_runtime': 1748.8667, 'train_samples_per_second': 109.785, 'train_steps_per_second': 3.431, 'total_flos': 5.051732262912e+16, 'train_loss': 0.13461771583557128, 'epoch': 3.0})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45a71501-23ba-4846-8b41-d574af0f8d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший порог: 0.4430\n",
      "Лучший F1: 0.6704\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(val_dataset)\n",
    "probs = softmax(preds.predictions, axis=1)[:, 1]\n",
    "\n",
    "\n",
    "thresholds = np.linspace(probs.min(), probs.max(), 100)\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    binary_preds = (probs > threshold).astype(int)\n",
    "    f1 = f1_score(y_val_np, binary_preds)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Лучший порог: {best_threshold:.4f}\")\n",
    "print(f\"Лучший F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9022bef-eb4a-41a6-b906-ed7f3383e924",
   "metadata": {},
   "source": [
    "### RoBERTa Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cabd9d3-d4d2-4807-ae17-c583e0d322a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148e837bf41b4d42bf16e0d51cb82235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd7c208e04643b6bd11fd5291b29a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975cdb2983d04ed8817913f0b7f60c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5821c7f1ede4ac9be7f39c0885c4cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c6eff557df4d55adfce914d88d8531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_robertal = AutoTokenizer.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "997a64ee-20b1-4496-a5d1-27ebd1c80438",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df, tokenizer_robertal)\n",
    "val_dataset = TextDataset(val_df, tokenizer_robertal)\n",
    "test_dataset = TextDataset(test_df, tokenizer_robertal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db30f04b-a193-400d-929f-06f2ffdb3396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900d3c4ad4014350bc4f4b248c1668a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_robertal = AutoModelForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "518b01f4-1af9-40d8-b2f0-fcae8b96ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./roberta-large_training_results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    metric_for_best_model='f1',\n",
    "    fp16=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c731728-5b07-4ce0-9558-f48556412436",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_robertal,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_train_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e41af262-9ef9-4059-843f-68652c6db69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 1:20:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.166600</td>\n",
       "      <td>0.195231</td>\n",
       "      <td>0.946467</td>\n",
       "      <td>0.745489</td>\n",
       "      <td>0.667186</td>\n",
       "      <td>0.919937</td>\n",
       "      <td>0.621491</td>\n",
       "      <td>0.720135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121600</td>\n",
       "      <td>0.205710</td>\n",
       "      <td>0.949305</td>\n",
       "      <td>0.759203</td>\n",
       "      <td>0.691515</td>\n",
       "      <td>0.927063</td>\n",
       "      <td>0.654000</td>\n",
       "      <td>0.733595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>0.239113</td>\n",
       "      <td>0.946066</td>\n",
       "      <td>0.752732</td>\n",
       "      <td>0.679660</td>\n",
       "      <td>0.927063</td>\n",
       "      <td>0.665591</td>\n",
       "      <td>0.694335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.12990824254353842, metrics={'train_runtime': 4831.0525, 'train_samples_per_second': 39.743, 'train_steps_per_second': 1.242, 'total_flos': 1.78930821758976e+17, 'train_loss': 0.12990824254353842, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecddd6b6-004d-4576-ba82-657d86524bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший порог: 0.4107\n",
      "Лучший F1: 0.6819\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "y_val_np = val_df['toxicity_b'].to_numpy()\n",
    "preds = trainer.predict(val_dataset)\n",
    "probs = softmax(preds.predictions, axis=1)[:, 1]\n",
    "\n",
    "\n",
    "thresholds = np.linspace(probs.min(), probs.max(), 100)\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    binary_preds = (probs > threshold).astype(int)\n",
    "    f1 = f1_score(y_val_np, binary_preds)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Лучший порог: {best_threshold:.4f}\")\n",
    "print(f\"Лучший F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b279ed8e-90bf-4add-b6e8-d1c5b74e5957",
   "metadata": {},
   "source": [
    "### DeBERTa V3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59576c7f-cbbb-486a-9da4-c99bd36edbd6",
   "metadata": {},
   "source": [
    "Эта модель обучается параметром TextDataset равным 448, в то время как все остальные модели обучались с параметром 512. Это связанно с тем, что модель не поместилась в видеопамять с параметром 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b759eb66-cba0-4ed5-9b87-d4a1f34b6a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lit-player/venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_deberta3 = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c186ee61-1079-4b1c-ace0-6b8976f72c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df, tokenizer_deberta3, max_length=504)\n",
    "val_dataset = TextDataset(val_df, tokenizer_deberta3, max_length=504)\n",
    "test_dataset = TextDataset(test_df, tokenizer_deberta3, max_length=504)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e44cd77-8d0c-44f0-ba67-b308156fd143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_deberta3 = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbf06d37-8b17-47ce-b8c8-6b5797f083a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./deberta3_training_results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    metric_for_best_model='f1',\n",
    "    fp16=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a27e1d5c-7322-4278-92eb-80f1592d5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_deberta3,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_train_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a3f15d2-e1a8-47d9-8b33-d6542e6e57c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 54:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.150400</td>\n",
       "      <td>0.196860</td>\n",
       "      <td>0.949212</td>\n",
       "      <td>0.761031</td>\n",
       "      <td>0.675436</td>\n",
       "      <td>0.919813</td>\n",
       "      <td>0.615207</td>\n",
       "      <td>0.748738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.117900</td>\n",
       "      <td>0.196205</td>\n",
       "      <td>0.950499</td>\n",
       "      <td>0.767450</td>\n",
       "      <td>0.679175</td>\n",
       "      <td>0.924125</td>\n",
       "      <td>0.642179</td>\n",
       "      <td>0.720695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.088300</td>\n",
       "      <td>0.212868</td>\n",
       "      <td>0.949917</td>\n",
       "      <td>0.762673</td>\n",
       "      <td>0.674986</td>\n",
       "      <td>0.925125</td>\n",
       "      <td>0.653705</td>\n",
       "      <td>0.697701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.1271060587565104, metrics={'train_runtime': 3281.4011, 'train_samples_per_second': 58.512, 'train_steps_per_second': 1.828, 'total_flos': 4.9728881276928e+16, 'train_loss': 0.1271060587565104, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd48764b-2f7e-4ca5-8fbb-de16cb18bffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший порог: 0.6635\n",
      "Лучший F1: 0.6775\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "y_val_np = val_df['toxicity_b'].to_numpy()\n",
    "preds = trainer.predict(val_dataset)\n",
    "probs = softmax(preds.predictions, axis=1)[:, 1]\n",
    "\n",
    "\n",
    "thresholds = np.linspace(probs.min(), probs.max(), 100)\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    binary_preds = (probs > threshold).astype(int)\n",
    "    f1 = f1_score(y_val_np, binary_preds)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Лучший порог: {best_threshold:.4f}\")\n",
    "print(f\"Лучший F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12229e79-1171-4f7c-b17c-21ee51048225",
   "metadata": {},
   "source": [
    "### DeBERTa V3 Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e52a2ad-16fa-421c-9a19-d13124be2c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lit-player/venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_deberta3l = AutoTokenizer.from_pretrained('microsoft/deberta-v3-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb93c9e0-ec75-4d88-8bff-55c5a5972565",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df, tokenizer_deberta3l, max_length=192)\n",
    "val_dataset = TextDataset(val_df, tokenizer_deberta3l, max_length=192)\n",
    "test_dataset = TextDataset(test_df, tokenizer_deberta3l, max_length=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ab7986e-9535-4432-9f9c-08bd8a3839f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_deberta3l = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-large', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faa7aa8d-eed4-4c05-b22c-45d64b2b71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./deberta3l_training_results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    metric_for_best_model='f1',\n",
    "    fp16=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26295471-f4d2-428d-93d9-46d0aedbcb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_deberta3l,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_train_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "922d7e12-e6f7-4d97-af82-e0e3ff9ba51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 48:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.174900</td>\n",
       "      <td>0.196433</td>\n",
       "      <td>0.937542</td>\n",
       "      <td>0.715870</td>\n",
       "      <td>0.651934</td>\n",
       "      <td>0.921250</td>\n",
       "      <td>0.642352</td>\n",
       "      <td>0.661806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.210829</td>\n",
       "      <td>0.922463</td>\n",
       "      <td>0.722788</td>\n",
       "      <td>0.655825</td>\n",
       "      <td>0.927250</td>\n",
       "      <td>0.693558</td>\n",
       "      <td>0.621985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113800</td>\n",
       "      <td>0.188305</td>\n",
       "      <td>0.942300</td>\n",
       "      <td>0.742174</td>\n",
       "      <td>0.675593</td>\n",
       "      <td>0.927312</td>\n",
       "      <td>0.672031</td>\n",
       "      <td>0.679192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.15766052627563476, metrics={'train_runtime': 2921.1628, 'train_samples_per_second': 65.727, 'train_steps_per_second': 2.054, 'total_flos': 6.7099511144448e+16, 'train_loss': 0.15766052627563476, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56f9df80-b864-40c1-8628-5fe74a08c0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший порог: 0.5851\n",
      "Лучший F1: 0.6768\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "y_val_np = val_df['toxicity_b'].to_numpy()\n",
    "preds = trainer.predict(val_dataset)\n",
    "probs = softmax(preds.predictions, axis=1)[:, 1]\n",
    "\n",
    "\n",
    "thresholds = np.linspace(probs.min(), probs.max(), 100)\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    binary_preds = (probs > threshold).astype(int)\n",
    "    f1 = f1_score(y_val_np, binary_preds)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Лучший порог: {best_threshold:.4f}\")\n",
    "print(f\"Лучший F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1778cc-8f5f-4496-b6c5-30317d88010d",
   "metadata": {},
   "source": [
    "### FP32 DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9832a96e-905b-4cfb-9fbf-f54d4a7b4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dbert = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f92fb822-b773-4930-a811-e5701f5fc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df, tokenizer_dbert)\n",
    "val_dataset = TextDataset(val_df, tokenizer_dbert)\n",
    "test_dataset = TextDataset(test_df, tokenizer_dbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2773f705-f66a-4206-9fb5-5ef16ac3f2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_dbert = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9de1ee0-c204-4547-88af-c70e885a6db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./dbert_training_results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    metric_for_best_model='f1',\n",
    "    #fp16=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "270f3834-ecc1-415e-aacf-600134c328f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_dbert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_train_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "209055a9-45ce-4465-ae41-6e94b68513af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 34:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.177036</td>\n",
       "      <td>0.943877</td>\n",
       "      <td>0.742914</td>\n",
       "      <td>0.662097</td>\n",
       "      <td>0.926063</td>\n",
       "      <td>0.674622</td>\n",
       "      <td>0.650028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>0.198450</td>\n",
       "      <td>0.943601</td>\n",
       "      <td>0.751792</td>\n",
       "      <td>0.663377</td>\n",
       "      <td>0.923250</td>\n",
       "      <td>0.648794</td>\n",
       "      <td>0.678632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.231514</td>\n",
       "      <td>0.942326</td>\n",
       "      <td>0.747733</td>\n",
       "      <td>0.662334</td>\n",
       "      <td>0.925312</td>\n",
       "      <td>0.667426</td>\n",
       "      <td>0.657319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.12476190185546875, metrics={'train_runtime': 2052.2772, 'train_samples_per_second': 93.555, 'train_steps_per_second': 2.924, 'total_flos': 2.5433740541952e+16, 'train_loss': 0.12476190185546875, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "692bda67-a9cc-41b6-8489-42b301d6e008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший порог: 0.3726\n",
      "Лучший F1: 0.6647\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "y_val_np = val_df['toxicity_b'].to_numpy()\n",
    "preds = trainer.predict(val_dataset)\n",
    "probs = softmax(preds.predictions, axis=1)[:, 1]\n",
    "\n",
    "\n",
    "thresholds = np.linspace(probs.min(), probs.max(), 100)\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    binary_preds = (probs > threshold).astype(int)\n",
    "    f1 = f1_score(y_val_np, binary_preds)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Лучший порог: {best_threshold:.4f}\")\n",
    "print(f\"Лучший F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb73ce-2d97-4c1a-9018-d56aa79d68b5",
   "metadata": {},
   "source": [
    "### Xlmr Large Toxicity Classifier V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf0ee420-3f2f-4b67-8b80-65403bb968e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "textdetox_tokenizer = AutoTokenizer.from_pretrained('textdetox/xlmr-large-toxicity-classifier-v2')\n",
    "textdetox_model = AutoModelForSequenceClassification.from_pretrained('textdetox/xlmr-large-toxicity-classifier-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82ce961f-1028-4a9a-9db7-8f5cccbd1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "textdetox_frozen_model = freeze_layers(textdetox_model, num_frozen_layers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8606e47-ddf5-4845-8591-5723ad28974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df, textdetox_tokenizer)\n",
    "val_dataset = TextDataset(val_df, textdetox_tokenizer)\n",
    "test_dataset = TextDataset(test_df, textdetox_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f4ecc5a-cf53-44b6-ba30-235582c0e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./textdetox_training_results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    fp16=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0b0c148-4f3f-4c7e-bf76-a43c62eb36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=textdetox_frozen_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_train_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "762d53be-8da5-4fa4-b946-8193cc505bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 1:21:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.169700</td>\n",
       "      <td>0.194411</td>\n",
       "      <td>0.931760</td>\n",
       "      <td>0.716111</td>\n",
       "      <td>0.641521</td>\n",
       "      <td>0.924562</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.605721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>0.222938</td>\n",
       "      <td>0.942511</td>\n",
       "      <td>0.737749</td>\n",
       "      <td>0.658549</td>\n",
       "      <td>0.917625</td>\n",
       "      <td>0.611940</td>\n",
       "      <td>0.712844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>0.238487</td>\n",
       "      <td>0.944360</td>\n",
       "      <td>0.742468</td>\n",
       "      <td>0.663518</td>\n",
       "      <td>0.924312</td>\n",
       "      <td>0.657489</td>\n",
       "      <td>0.669658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.13873130416870116, metrics={'train_runtime': 4873.6608, 'train_samples_per_second': 39.395, 'train_steps_per_second': 1.231, 'total_flos': 1.78930821758976e+17, 'train_loss': 0.13873130416870116, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0bf190d-e3cc-4284-82ef-bda142d7885c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший порог: 0.5217\n",
      "Лучший F1: 0.6650\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "y_val_np = val_df['toxicity_b'].to_numpy()\n",
    "preds = trainer.predict(val_dataset)\n",
    "probs = softmax(preds.predictions, axis=1)[:, 1]\n",
    "\n",
    "\n",
    "thresholds = np.linspace(probs.min(), probs.max(), 100)\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    binary_preds = (probs > threshold).astype(int)\n",
    "    f1 = f1_score(y_val_np, binary_preds)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Лучший порог: {best_threshold:.4f}\")\n",
    "print(f\"Лучший F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b9d6f-3355-4fa8-a5c1-d0792562ac56",
   "metadata": {},
   "source": [
    "### Hate-speech-CNERG/dehatebert-mono-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b621ad62-0a74-48b6-a1fb-3f6288fd4013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dced89f76d5f46a789992075977013a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/152 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20710e62e0d4430cb270e83dd916035f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c60d7356ba6465eb76616477493a042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc21715c2e04b849353361e25da35ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92f53e8ea3d44dcbccca611579ccb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d875fe282fb45f8b14ad9ae8a21805a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hs_tokenizer = AutoTokenizer.from_pretrained('Hate-speech-CNERG/dehatebert-mono-english')\n",
    "hs_model = AutoModelForSequenceClassification.from_pretrained('Hate-speech-CNERG/dehatebert-mono-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b58f1f6d-a3fa-4d83-8032-229f26933da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df, hs_tokenizer)\n",
    "val_dataset = TextDataset(val_df, hs_tokenizer)\n",
    "test_dataset = TextDataset(test_df, hs_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36a9f4a1-2f54-4609-a953-a5ed1afb094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./hs_training_results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    fp16=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfc3427d-bd26-4068-8f25-f71b40c928c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=hs_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_train_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fc9db65-7aff-484a-8c43-b6f706b9875f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 28:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.195900</td>\n",
       "      <td>0.245054</td>\n",
       "      <td>0.895098</td>\n",
       "      <td>0.621167</td>\n",
       "      <td>0.568571</td>\n",
       "      <td>0.905625</td>\n",
       "      <td>0.579499</td>\n",
       "      <td>0.558048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.247160</td>\n",
       "      <td>0.906541</td>\n",
       "      <td>0.658129</td>\n",
       "      <td>0.598286</td>\n",
       "      <td>0.909188</td>\n",
       "      <td>0.589967</td>\n",
       "      <td>0.606842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.165500</td>\n",
       "      <td>0.238985</td>\n",
       "      <td>0.909721</td>\n",
       "      <td>0.664351</td>\n",
       "      <td>0.599116</td>\n",
       "      <td>0.914937</td>\n",
       "      <td>0.630893</td>\n",
       "      <td>0.570387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.1863182856241862, metrics={'train_runtime': 1720.8099, 'train_samples_per_second': 111.575, 'train_steps_per_second': 3.487, 'total_flos': 5.051732262912e+16, 'train_loss': 0.1863182856241862, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0fffb1c-b0bd-4841-b6b8-c4615be079b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший порог: 0.3623\n",
      "Лучший F1: 0.6110\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "y_val_np = val_df['toxicity_b'].to_numpy()\n",
    "preds = trainer.predict(val_dataset)\n",
    "probs = softmax(preds.predictions, axis=1)[:, 1]\n",
    "\n",
    "\n",
    "thresholds = np.linspace(probs.min(), probs.max(), 100)\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    binary_preds = (probs > threshold).astype(int)\n",
    "    f1 = f1_score(y_val_np, binary_preds)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Лучший порог: {best_threshold:.4f}\")\n",
    "print(f\"Лучший F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f0c98-73db-4289-b747-f7c2b7bd7ebb",
   "metadata": {},
   "source": [
    "## Обучение моделей на полных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "387a640e-2ebf-4a79-b097-1de32a7f6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.loc[:1428245], df.loc[1428246:]\n",
    "train_df, val_df = train_df.loc[:1142596], train_df.loc[1142597:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e3f11a-d988-4f5c-a01d-233e5834b844",
   "metadata": {},
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3c4ee7e-1560-4bad-a3a4-4bcfd3386d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dbert = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "262749eb-db3e-4f8d-b756-3cd65ea51e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df, tokenizer_dbert)\n",
    "val_dataset = TextDataset(val_df, tokenizer_dbert)\n",
    "test_dataset = TextDataset(test_df, tokenizer_dbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d70dcb0d-eda1-47d9-81db-36791031ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_dbert = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1e11d67-2b83-4afb-badd-5c4e11462a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./dbert_training_results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    metric_for_best_model='f1',\n",
    "    fp16=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "844f22fa-229c-4596-9cb0-170fdd58bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_dbert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_train_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47a18cb6-72c3-4995-83a2-92899bd72028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='107121' max='107121' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [107121/107121 4:24:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.161231</td>\n",
       "      <td>0.959167</td>\n",
       "      <td>0.803648</td>\n",
       "      <td>0.709643</td>\n",
       "      <td>0.935225</td>\n",
       "      <td>0.742854</td>\n",
       "      <td>0.679274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>0.162525</td>\n",
       "      <td>0.960119</td>\n",
       "      <td>0.806491</td>\n",
       "      <td>0.714649</td>\n",
       "      <td>0.935655</td>\n",
       "      <td>0.739470</td>\n",
       "      <td>0.691441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.187396</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.797685</td>\n",
       "      <td>0.709071</td>\n",
       "      <td>0.934157</td>\n",
       "      <td>0.730844</td>\n",
       "      <td>0.688557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=107121, training_loss=0.13017315996813733, metrics={'train_runtime': 15894.8279, 'train_samples_per_second': 215.654, 'train_steps_per_second': 6.739, 'total_flos': 4.540705569064489e+17, 'train_loss': 0.13017315996813733, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e1cde97-8d23-475f-82c2-277feb2b3f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший порог: 0.3837\n",
      "Лучший F1: 0.7110\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "y_val_np = val_df['toxicity_b'].to_numpy()\n",
    "preds = trainer.predict(val_dataset)\n",
    "probs = softmax(preds.predictions, axis=1)[:, 1]\n",
    "\n",
    "\n",
    "thresholds = np.linspace(probs.min(), probs.max(), 100)\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    binary_preds = (probs > threshold).astype(int)\n",
    "    f1 = f1_score(y_val_np, binary_preds)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Лучший порог: {best_threshold:.4f}\")\n",
    "print(f\"Лучший F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff96c3-9aa4-4670-b141-1f6622171bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
